{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5dd89c97149442cbad0a7ced9ad692c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5137c977756c4694bec28ba157e8dea9",
              "IPY_MODEL_1ea73ddb674c413faad62084da14ac31",
              "IPY_MODEL_5ad9a924f0704291ab8d8d155923a755"
            ],
            "layout": "IPY_MODEL_fee22c7532b546a39d74085caede4bed"
          }
        },
        "5137c977756c4694bec28ba157e8dea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3477b2ffb4084bf38fcbbcef31492da3",
            "placeholder": "​",
            "style": "IPY_MODEL_da96756ec461400aaeb8771b70873160",
            "value": "Loading weights: 100%"
          }
        },
        "1ea73ddb674c413faad62084da14ac31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d24f83caafb439db38a6967d669a365",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5d2814b33194e3fb711dc8cb6a332b5",
            "value": 103
          }
        },
        "5ad9a924f0704291ab8d8d155923a755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_731868d8883c4d24a926a04503a7bc6e",
            "placeholder": "​",
            "style": "IPY_MODEL_ae9e370ac19b4dea9bd3adc7a263998c",
            "value": " 103/103 [00:00&lt;00:00, 352.63it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "fee22c7532b546a39d74085caede4bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3477b2ffb4084bf38fcbbcef31492da3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da96756ec461400aaeb8771b70873160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d24f83caafb439db38a6967d669a365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5d2814b33194e3fb711dc8cb6a332b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "731868d8883c4d24a926a04503a7bc6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae9e370ac19b4dea9bd3adc7a263998c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y google-adk langchain langchain-text-splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6vfX-kPR4isU",
        "outputId": "575b447a-b430-488c-b5e6-c74f909b6b8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping google-adk as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: langchain 1.2.9\n",
            "Uninstalling langchain-1.2.9:\n",
            "  Successfully uninstalled langchain-1.2.9\n",
            "Found existing installation: langchain-text-splitters 1.1.0\n",
            "Uninstalling langchain-text-splitters-1.1.0:\n",
            "  Successfully uninstalled langchain-text-splitters-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v_utqLMUfi6p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHsKup-2Cojy",
        "outputId": "cec534ce-ef93-451a-f0b6-1b3ece65b553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.2/111.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/496.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m491.5/496.3 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.3/496.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U chromadb sentence-transformers rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"db\")\n",
        "\n",
        "dense_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"paul_graham\",\n",
        "    embedding_function=dense_ef\n",
        ")\n",
        "\n",
        "# 1) Load dataset safely\n",
        "ds = load_dataset(\"chromadb/paul_graham_essay\", split=\"data\", streaming=True)\n",
        "\n",
        "# 2) Build text without pulling everything into RAM\n",
        "#    Also cap total chars to prevent runaway chunk counts\n",
        "MAX_CHARS = 250_000  # adjust up if you want, but start here\n",
        "texts = []\n",
        "total = 0\n",
        "for row in ds:\n",
        "    # pick a text field robustly\n",
        "    if \"text\" in row:\n",
        "        t = row[\"text\"]\n",
        "    else:\n",
        "        # first string-ish field\n",
        "        t = next(v for v in row.values() if isinstance(v, str))\n",
        "    if not t:\n",
        "        continue\n",
        "    if total + len(t) > MAX_CHARS:\n",
        "        t = t[: max(0, MAX_CHARS - total)]\n",
        "    texts.append(t)\n",
        "    total += len(t)\n",
        "    if total >= MAX_CHARS:\n",
        "        break\n",
        "\n",
        "text = \"\\n\".join(texts)\n",
        "\n",
        "# 3) Chunk\n",
        "def chunk_text(s, chunk_size=1200, overlap=200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(s)\n",
        "    while start < n:\n",
        "        end = min(n, start + chunk_size)\n",
        "        chunk = s[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "        if end == n:\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(text, chunk_size=1200, overlap=200)\n",
        "\n",
        "# 4) Batched add (prevents big embedding spikes)\n",
        "def batched_add(coll, chunks, batch_size=64):\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        ids = [f\"pg_{j}\" for j in range(i, i+len(batch))]\n",
        "        metas = [{\"source\": \"chromadb/paul_graham_essay\", \"chunk\": j} for j in range(i, i+len(batch))]\n",
        "        coll.add(ids=ids, documents=batch, metadatas=metas)\n",
        "\n",
        "batched_add(collection, chunks, batch_size=64)\n",
        "\n",
        "collection.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "5dd89c97149442cbad0a7ced9ad692c4",
            "5137c977756c4694bec28ba157e8dea9",
            "1ea73ddb674c413faad62084da14ac31",
            "5ad9a924f0704291ab8d8d155923a755",
            "fee22c7532b546a39d74085caede4bed",
            "3477b2ffb4084bf38fcbbcef31492da3",
            "da96756ec461400aaeb8771b70873160",
            "7d24f83caafb439db38a6967d669a365",
            "e5d2814b33194e3fb711dc8cb6a332b5",
            "731868d8883c4d24a926a04503a7bc6e",
            "ae9e370ac19b4dea9bd3adc7a263998c"
          ]
        },
        "id": "z87JNopm9Bd8",
        "outputId": "41d34f6a-0596-449c-b160-413942c215b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5dd89c97149442cbad0a7ced9ad692c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Collection management (list / get / rename-by-copy / delete)"
      ],
      "metadata": {
        "id": "Jc8QsKdL_PCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "# List collections\n",
        "print([c.name for c in client.list_collections()])\n",
        "\n",
        "# Get the existing collection safely (reuses embedding function)\n",
        "collection = client.get_collection(\"paul_graham\", embedding_function=dense_ef)\n",
        "\n",
        "print(\"count:\", collection.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKZuIMsj_Gyk",
        "outputId": "5fd00830-0a06-47a4-e6e9-bb10bd5b3b56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['paul_graham', 'demo_dense']\n",
            "count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename is not a direct operation; do copy + delete:"
      ],
      "metadata": {
        "id": "Kne3JeUb_XJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_collection(client, old_name, new_name, embedding_function=None, batch_size=256):\n",
        "    old = client.get_collection(old_name, embedding_function=embedding_function)\n",
        "    new = client.get_or_create_collection(new_name, embedding_function=embedding_function)\n",
        "\n",
        "    data = old.get(include=[\"documents\",\"metadatas\"])\n",
        "    ids = data[\"ids\"]\n",
        "    docs = data[\"documents\"]\n",
        "    metas = data[\"metadatas\"]\n",
        "\n",
        "    for i in range(0, len(ids), batch_size):\n",
        "        new.add(\n",
        "            ids=ids[i:i+batch_size],\n",
        "            documents=docs[i:i+batch_size],\n",
        "            metadatas=metas[i:i+batch_size],\n",
        "        )\n",
        "\n",
        "    client.delete_collection(old_name)\n",
        "    return new\n",
        "\n",
        "# Example:\n",
        "# collection = rename_collection(client, \"paul_graham\", \"paul_graham_v2\", embedding_function=dense_ef)"
      ],
      "metadata": {
        "id": "4W3R-2Qk_HqM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Updating & removing data (update / upsert / delete-by-metadata)\n",
        "\n",
        "Chroma supports `update()` and `delete()`. Some versions also support `upsert()`; this wrapper works either way."
      ],
      "metadata": {
        "id": "3_Io5mDL_aN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_upsert(coll, ids, documents=None, metadatas=None):\n",
        "    # If upsert exists, use it; else do update then add missing ids.\n",
        "    if hasattr(coll, \"upsert\"):\n",
        "        return coll.upsert(ids=ids, documents=documents, metadatas=metadatas)\n",
        "\n",
        "    existing = set(coll.get(ids=ids, include=[]).get(\"ids\", []))\n",
        "    to_update = [i for i in ids if i in existing]\n",
        "    to_add = [i for i in ids if i not in existing]\n",
        "\n",
        "    if to_update:\n",
        "        idx = [ids.index(i) for i in to_update]\n",
        "        coll.update(\n",
        "            ids=to_update,\n",
        "            documents=[documents[j] for j in idx] if documents else None,\n",
        "            metadatas=[metadatas[j] for j in idx] if metadatas else None,\n",
        "        )\n",
        "    if to_add:\n",
        "        idx = [ids.index(i) for i in to_add]\n",
        "        coll.add(\n",
        "            ids=to_add,\n",
        "            documents=[documents[j] for j in idx] if documents else None,\n",
        "            metadatas=[metadatas[j] for j in idx] if metadatas else None,\n",
        "        )\n",
        "\n",
        "# Example: update an existing chunk\n",
        "collection.update(\n",
        "    ids=[\"pg_0\"],\n",
        "    documents=[\"[UPDATED] \" + collection.get(ids=[\"pg_0\"], include=[\"documents\"])[\"documents\"][0]],\n",
        "    metadatas=[{\"source\":\"chromadb/paul_graham_essay\",\"chunk\":0,\"updated\":True}]\n",
        ")\n",
        "\n",
        "# Example: delete by ids\n",
        "collection.delete(ids=[\"pg_1\",\"pg_2\"])\n",
        "\n",
        "print(\"count:\", collection.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5itZ5lM_Laa",
        "outputId": "75da4fdd-59e8-4548-fe82-a93b33df8adb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# delete all records where updated=True\n",
        "collection.delete(where={\"updated\": True})\n",
        "print(\"count:\", collection.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PtUp9lr_dPC",
        "outputId": "6261edc9-f550-4738-a67f-928ead818a42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dense search (semantic)"
      ],
      "metadata": {
        "id": "cGwyLGfu_t-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_search(query, k=5):\n",
        "    r = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=k,\n",
        "        include=[\"documents\",\"metadatas\",\"distances\"]\n",
        "    )\n",
        "    out = []\n",
        "    for _id, doc, meta, dist in zip(r[\"ids\"][0], r[\"documents\"][0], r[\"metadatas\"][0], r[\"distances\"][0]):\n",
        "        out.append({\"id\": _id, \"distance\": float(dist), \"meta\": meta, \"preview\": doc[:180]})\n",
        "    return out\n",
        "\n",
        "dense_search(\"advice about learning fast and writing\", k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvR12VDK_rul",
        "outputId": "c3df4a10-6942-4092-b971-950446c9e04b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lexical search (BM25) + rebuild after changes"
      ],
      "metadata": {
        "id": "zjaRfcyk_6SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"count:\", collection.count())\n",
        "sample = collection.get(limit=3, include=[\"documents\",\"metadatas\"])\n",
        "print(\"ids:\", len(sample[\"ids\"]))\n",
        "print(\"docs:\", len(sample.get(\"documents\") or []))\n",
        "print(\"example doc:\", (sample.get(\"documents\") or [None])[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ5W3ccXAPbS",
        "outputId": "db483493-5352-4325-f043-c0ef5ec05ecb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 0\n",
            "ids: 0\n",
            "docs: 0\n",
            "example doc: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# (Re)create clean collection if you want\n",
        "# client.delete_collection(\"paul_graham\")\n",
        "# collection = client.get_or_create_collection(\"paul_graham\", embedding_function=dense_ef)\n",
        "\n",
        "# Load the dataset (split is \"data\")\n",
        "ds = load_dataset(\"chromadb/paul_graham_essay\", split=\"data\", streaming=True)\n",
        "\n",
        "# Build text (cap to avoid too many chunks)\n",
        "MAX_CHARS = 250_000\n",
        "texts, total = [], 0\n",
        "for row in ds:\n",
        "    t = row.get(\"text\") or next(v for v in row.values() if isinstance(v, str))\n",
        "    if not t:\n",
        "        continue\n",
        "    if total + len(t) > MAX_CHARS:\n",
        "        t = t[: max(0, MAX_CHARS - total)]\n",
        "    texts.append(t)\n",
        "    total += len(t)\n",
        "    if total >= MAX_CHARS:\n",
        "        break\n",
        "\n",
        "text = \"\\n\".join(texts)\n",
        "\n",
        "def chunk_text(s, chunk_size=1200, overlap=200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(s)\n",
        "    while start < n:\n",
        "        end = min(n, start + chunk_size)\n",
        "        chunk = s[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "        if end == n:\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(text, chunk_size=1200, overlap=200)\n",
        "print(\"chars:\", len(text), \"chunks:\", len(chunks))\n",
        "\n",
        "def batched_add(coll, chunks, batch_size=64):\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        ids = [f\"pg_{j}\" for j in range(i, i+len(batch))]\n",
        "        metas = [{\"source\": \"chromadb/paul_graham_essay\", \"chunk\": j} for j in range(i, i+len(batch))]\n",
        "        coll.add(ids=ids, documents=batch, metadatas=metas)\n",
        "\n",
        "batched_add(collection, chunks, batch_size=64)\n",
        "\n",
        "print(\"collection count:\", collection.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbHhzeVpAb3I",
        "outputId": "4f0a6f43-45b0-4742-f500-2ead71405fc5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chars: 305 chunks: 1\n",
            "collection count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import re\n",
        "\n",
        "def tokenize(text: str):\n",
        "    return re.findall(r\"[a-z0-9]+\", text.lower())\n",
        "\n",
        "def rebuild_bm25(coll):\n",
        "    data = coll.get(include=[\"documents\"])\n",
        "    ids = data[\"ids\"]\n",
        "    docs = data[\"documents\"]\n",
        "    bm25 = BM25Okapi([tokenize(d) for d in docs])\n",
        "    return bm25, ids, docs\n",
        "\n",
        "bm25, bm25_ids, bm25_docs = rebuild_bm25(collection)\n",
        "\n",
        "def bm25_search(query, k=5):\n",
        "    scores = bm25.get_scores(tokenize(query))\n",
        "    ranked = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [{\"id\": bm25_ids[i], \"score\": float(scores[i]), \"preview\": bm25_docs[i][:180]} for i in ranked]\n",
        "\n",
        "bm25_search(\"venture capital startup\", k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQWnZsSz_v8X",
        "outputId": "9128052e-862f-46cd-ac6a-3ceebc49498c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'pg_0',\n",
              "  'score': 0.0,\n",
              "  'preview': '0\\n79\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n1\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\n62\\n6'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hybrid search (dense + lexical) using your existing RRF code\n",
        "\n",
        "You already have dense_ranked_ids, rrf, hybrid_search. You just need bm25_search returning ids. Add this helper:"
      ],
      "metadata": {
        "id": "n1PHkbb3AGyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_search_ids(query, k=10):\n",
        "    scores = bm25.get_scores(tokenize(query))\n",
        "    ranked = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [bm25_ids[i] for i in ranked]"
      ],
      "metadata": {
        "id": "12oW-ajs_9Az"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_ranked_ids(query, k=10):\n",
        "    r = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=k\n",
        "    )\n",
        "    return r[\"ids\"][0]\n",
        "\n",
        "\n",
        "def bm25_search_ids(query, k=10):\n",
        "    if bm25 is None:\n",
        "        return []\n",
        "    scores = bm25.get_scores(tokenize(query))\n",
        "    ranked = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [bm25_ids[i] for i in ranked]\n",
        "\n",
        "\n",
        "def rrf(rank_lists, k=60, weights=None):\n",
        "    if weights is None:\n",
        "        weights = [1.0] * len(rank_lists)\n",
        "    scores = {}\n",
        "    for w, ids in zip(weights, rank_lists):\n",
        "        for rank, _id in enumerate(ids):\n",
        "            scores[_id] = scores.get(_id, 0.0) + w * (1.0 / (k + rank))\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "KSadcNDqA4yi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_search(query, k_dense=10, k_lex=10, k_final=5):\n",
        "    dense_ids = dense_ranked_ids(query, k=k_dense)\n",
        "    lex_ids = bm25_search_ids(query, k=k_lex)\n",
        "    fused = rrf([dense_ids, lex_ids], k=60)\n",
        "\n",
        "    top = [i for i, _ in fused[:k_final]]\n",
        "    got = collection.get(ids=top, include=[\"documents\",\"metadatas\"])\n",
        "    id_to_idx = {i: idx for idx, i in enumerate(got[\"ids\"])}\n",
        "\n",
        "    out = []\n",
        "    for _id in top:\n",
        "        idx = id_to_idx[_id]\n",
        "        out.append((_id, got[\"metadatas\"][idx], got[\"documents\"][idx][:160]))\n",
        "    return out\n",
        "\n",
        "\n",
        "hybrid_search(\"startup advice about writing and learning\", k_final=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6GOedWiA8-O",
        "outputId": "020d2299-6ab6-4caf-d8bb-1a610f7e28fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pg_0',\n",
              "  {'chunk': 0, 'source': 'chromadb/paul_graham_essay'},\n",
              "  '0\\n79\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n1\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimal “workflow” pattern you’ll use in the notebook"
      ],
      "metadata": {
        "id": "i_Rty277BIta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25, bm25_ids, bm25_docs = rebuild_bm25(collection)"
      ],
      "metadata": {
        "id": "YDMQQaiCAu1B"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_search(\"...\", k=5)\n",
        "bm25_search(\"...\", k=5)\n",
        "hybrid_search(\"...\", k_final=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do5JRKDtBN5A",
        "outputId": "1b42c0eb-9be8-49c2-a0be-33087fc5aab6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pg_0',\n",
              "  {'chunk': 0, 'source': 'chromadb/paul_graham_essay'},\n",
              "  '0\\n79\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n1\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------\n",
        "##Metadata Filtering (pre-filter)\n",
        "\n",
        "Decide what metadata is “filterable”\n",
        "\n",
        "In real systems this is not arbitrary.\n",
        "\n",
        "Typical filter dimensions:\n",
        "\n",
        "- source (document / dataset / tenant)\n",
        "\n",
        "- chunk or doc_id\n",
        "\n",
        "- section, date, author\n",
        "\n",
        "- access_level, user_id, org_id\n",
        "\n",
        "We’ll use: `source` and `chunk (numeric)`\n",
        "\n",
        "## Dense search WITH pre-filter (semantic + metadata)\n",
        "\n",
        "This is the most common production pattern."
      ],
      "metadata": {
        "id": "qP0ETdBKCBSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_search_filtered(query, where, k=5):\n",
        "    r = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=k,\n",
        "        where=where,\n",
        "        include=[\"documents\",\"metadatas\",\"distances\"]\n",
        "    )\n",
        "    out = []\n",
        "    for _id, doc, meta, dist in zip(\n",
        "        r[\"ids\"][0], r[\"documents\"][0], r[\"metadatas\"][0], r[\"distances\"][0]\n",
        "    ):\n",
        "        out.append({\n",
        "            \"id\": _id,\n",
        "            \"distance\": float(dist),\n",
        "            \"meta\": meta,\n",
        "            \"preview\": doc[:160]\n",
        "        })\n",
        "    return out"
      ],
      "metadata": {
        "id": "smEj1GL7BQed"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example usage:"
      ],
      "metadata": {
        "id": "jtUHwOonClBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense_search_filtered(\n",
        "    \"startup advice about writing\",\n",
        "    where={\"source\": \"chromadb/paul_graham_essay\"},\n",
        "    k=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33tiz9n9ChGi",
        "outputId": "22b427fb-e42d-49d5-ab2d-205faff4d6b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'pg_0',\n",
              "  'distance': 1.0570777654647827,\n",
              "  'meta': {'chunk': 0, 'source': 'chromadb/paul_graham_essay'},\n",
              "  'preview': '0\\n79\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n1\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56'}]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key point:**\n",
        "Filtering happens before vector similarity → faster + correct."
      ],
      "metadata": {
        "id": "8XqlaT6DCrof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lexical (BM25) WITH pre-filter"
      ],
      "metadata": {
        "id": "IKE9OWX8CvLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_search_filtered(query, where, k=5):\n",
        "    if bm25 is None:\n",
        "        return []\n",
        "\n",
        "    filtered = [\n",
        "        (i, bm25_docs[i])\n",
        "        for i in range(len(bm25_docs))\n",
        "        if all(bm25_docs[i] is not None and\n",
        "               collection.get(ids=[bm25_ids[i]], include=[\"metadatas\"])[\"metadatas\"][0].get(key) == val\n",
        "               for key, val in where.items())\n",
        "    ]\n",
        "\n",
        "    if not filtered:\n",
        "        return []\n",
        "\n",
        "    idxs, docs = zip(*filtered)\n",
        "    scores = bm25.get_scores(tokenize(query))\n",
        "    ranked = sorted(idxs, key=lambda i: scores[i], reverse=True)[:k]\n",
        "\n",
        "    return [\n",
        "        {\"id\": bm25_ids[i], \"score\": float(scores[i]), \"preview\": bm25_docs[i][:160]}\n",
        "        for i in ranked\n",
        "    ]"
      ],
      "metadata": {
        "id": "WAfdY6kzCn4u"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of usage:"
      ],
      "metadata": {
        "id": "Td04Zj0eC1Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_search_filtered(\n",
        "    \"venture capital\",\n",
        "    where={\"source\": \"chromadb/paul_graham_essay\"},\n",
        "    k=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7QqpYZNCygR",
        "outputId": "aaca64a3-702b-4d84-b1c8-d20cb6ac0d9c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'pg_0',\n",
              "  'score': 0.0,\n",
              "  'preview': '0\\n79\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n1\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid search WITH pre-filter (real-world default)\n",
        "\n",
        "This is the industry-standard configuration:\n",
        "\n",
        "filter → recall → rerank → fuse"
      ],
      "metadata": {
        "id": "1nPxVYb1C6vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_search_filtered(query, where, k_dense=10, k_lex=10, k_final=5):\n",
        "    # Dense (already supports pre-filter)\n",
        "    dense_ids = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=k_dense,\n",
        "        where=where\n",
        "    )[\"ids\"][0]\n",
        "\n",
        "    # Lexical (manual filter)\n",
        "    lex_ids = [\n",
        "        r[\"id\"]\n",
        "        for r in bm25_search_filtered(query, where=where, k=k_lex)\n",
        "    ]\n",
        "\n",
        "    fused = rrf([dense_ids, lex_ids], k=60)\n",
        "    top = [i for i, _ in fused[:k_final]]\n",
        "\n",
        "    got = collection.get(ids=top, include=[\"documents\",\"metadatas\"])\n",
        "    id_to_idx = {i: idx for idx, i in enumerate(got[\"ids\"])}\n",
        "\n",
        "    return [\n",
        "        (_id, got[\"metadatas\"][id_to_idx[_id]], got[\"documents\"][id_to_idx[_id]][:160])\n",
        "        for _id in top\n",
        "    ]"
      ],
      "metadata": {
        "id": "NsBZkdkbC3MF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_search_filtered(\n",
        "    \"startup advice about writing and learning\",\n",
        "    where={\"source\": \"chromadb/paul_graham_essay\"},\n",
        "    k_final=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z7b8Zi-C_Cb",
        "outputId": "234b4125-a626-42a5-93b5-41d0f4c32d34"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pg_0',\n",
              "  {'chunk': 0, 'source': 'chromadb/paul_graham_essay'},\n",
              "  '0\\n79\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n1\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo comparison"
      ],
      "metadata": {
        "id": "6d6bjYBtDbLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collection.add(\n",
        "    ids=[f\"other_{i}\" for i in range(5)],\n",
        "    documents=[\n",
        "        \"This document is about cooking and baking.\",\n",
        "        \"This document is about skincare routines.\",\n",
        "        \"This document is about cloud architecture.\",\n",
        "        \"This document is about accounting standards.\",\n",
        "        \"This document is about vector databases.\",\n",
        "    ],\n",
        "    metadatas=[{\"source\":\"other\", \"chunk\": i} for i in range(5)]\n",
        ")"
      ],
      "metadata": {
        "id": "L8J-tE86Dnmi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_search(\"startup advice\", k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRyEAFnpDBKZ",
        "outputId": "717d3410-c5b4-4da3-89e1-d52d8c980185"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'other_0',\n",
              "  'distance': 0.9585310220718384,\n",
              "  'meta': {'chunk': 0, 'source': 'other'},\n",
              "  'preview': 'This document is about cooking and baking.'},\n",
              " {'id': 'other_2',\n",
              "  'distance': 0.9839558601379395,\n",
              "  'meta': {'source': 'other', 'chunk': 2},\n",
              "  'preview': 'This document is about cloud architecture.'},\n",
              " {'id': 'other_1',\n",
              "  'distance': 0.9861060380935669,\n",
              "  'meta': {'source': 'other', 'chunk': 1},\n",
              "  'preview': 'This document is about skincare routines.'},\n",
              " {'id': 'other_4',\n",
              "  'distance': 1.0113791227340698,\n",
              "  'meta': {'source': 'other', 'chunk': 4},\n",
              "  'preview': 'This document is about vector databases.'},\n",
              " {'id': 'other_3',\n",
              "  'distance': 1.0553466081619263,\n",
              "  'meta': {'chunk': 3, 'source': 'other'},\n",
              "  'preview': 'This document is about accounting standards.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def dense_search_filtered_timed(query, where=None, k=5, repeat=1):\n",
        "    # warm-up\n",
        "    if where is None:\n",
        "        collection.query(query_texts=[query], n_results=1)\n",
        "    else:\n",
        "        collection.query(query_texts=[query], n_results=1, where=where)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(repeat):\n",
        "        if where is None:\n",
        "            r = collection.query(\n",
        "                query_texts=[query],\n",
        "                n_results=k,\n",
        "                include=[\"documents\",\"metadatas\",\"distances\"]\n",
        "            )\n",
        "        else:\n",
        "            r = collection.query(\n",
        "                query_texts=[query],\n",
        "                n_results=k,\n",
        "                where=where,\n",
        "                include=[\"documents\",\"metadatas\",\"distances\"]\n",
        "            )\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    elapsed_ms = (t1 - t0) * 1000 / repeat\n",
        "\n",
        "    out = []\n",
        "    for _id, doc, meta, dist in zip(\n",
        "        r[\"ids\"][0], r[\"documents\"][0], r[\"metadatas\"][0], r[\"distances\"][0]\n",
        "    ):\n",
        "        out.append({\n",
        "            \"id\": _id,\n",
        "            \"distance\": float(dist),\n",
        "            \"meta\": meta,\n",
        "            \"preview\": doc[:160]\n",
        "        })\n",
        "\n",
        "    print(f\"where={where if where is not None else 'NO FILTER'} | avg latency: {elapsed_ms:.2f} ms\")\n",
        "    return out"
      ],
      "metadata": {
        "id": "6Lkx3zuFD0AJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_search_filtered_timed(\n",
        "    \"startup advice\",\n",
        "    where={\"source\":\"chromadb/paul_graham_essay\"},\n",
        "    k=5,\n",
        "    repeat=5\n",
        ")\n",
        "\n",
        "dense_search_filtered_timed(\n",
        "    \"startup advice\",\n",
        "    where={\"source\":\"other\"},\n",
        "    k=5,\n",
        "    repeat=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbrxsLFcD4ZF",
        "outputId": "5aaf6f93-ae2b-4773-8caa-7359575f95ac"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where={'source': 'chromadb/paul_graham_essay'} | avg latency: 16.01 ms\n",
            "where={'source': 'other'} | avg latency: 16.35 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'other_0',\n",
              "  'distance': 0.9585310220718384,\n",
              "  'meta': {'source': 'other', 'chunk': 0},\n",
              "  'preview': 'This document is about cooking and baking.'},\n",
              " {'id': 'other_2',\n",
              "  'distance': 0.9839558601379395,\n",
              "  'meta': {'chunk': 2, 'source': 'other'},\n",
              "  'preview': 'This document is about cloud architecture.'},\n",
              " {'id': 'other_1',\n",
              "  'distance': 0.9861060380935669,\n",
              "  'meta': {'chunk': 1, 'source': 'other'},\n",
              "  'preview': 'This document is about skincare routines.'},\n",
              " {'id': 'other_4',\n",
              "  'distance': 1.0113791227340698,\n",
              "  'meta': {'source': 'other', 'chunk': 4},\n",
              "  'preview': 'This document is about vector databases.'},\n",
              " {'id': 'other_3',\n",
              "  'distance': 1.0553466081619263,\n",
              "  'meta': {'source': 'other', 'chunk': 3},\n",
              "  'preview': 'This document is about accounting standards.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# no filter\n",
        "dense_search_filtered_timed(\n",
        "    \"startup advice\",\n",
        "    where=None,\n",
        "    k=5,\n",
        "    repeat=5\n",
        ")\n",
        "\n",
        "# with filter\n",
        "dense_search_filtered_timed(\n",
        "    \"startup advice\",\n",
        "    where={\"source\":\"chromadb/paul_graham_essay\"},\n",
        "    k=5,\n",
        "    repeat=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEipYFF2EQQv",
        "outputId": "24cd268b-1044-42ed-e0ba-24b439a5cc1d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where=NO FILTER | avg latency: 16.18 ms\n",
            "where={'source': 'chromadb/paul_graham_essay'} | avg latency: 16.07 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'pg_0',\n",
              "  'distance': 1.0795005559921265,\n",
              "  'meta': {'source': 'chromadb/paul_graham_essay', 'chunk': 0},\n",
              "  'preview': '0\\n79\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n1\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56'}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv8oIS5WDhbz",
        "outputId": "313061be-ae4d-4142-cf97-85d4096376a4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'pg_0',\n",
              "  'distance': 1.0795005559921265,\n",
              "  'meta': {'source': 'chromadb/paul_graham_essay', 'chunk': 0},\n",
              "  'preview': '0\\n79\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n1\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56'}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def candidate_count(where=None):\n",
        "    if where is None:\n",
        "        return len(collection.get(include=[]).get(\"ids\", []))\n",
        "    return len(collection.get(where=where, include=[]).get(\"ids\", []))\n",
        "\n",
        "print(\"All candidates:\", candidate_count())\n",
        "print(\"PG candidates:\", candidate_count({\"source\":\"chromadb/paul_graham_essay\"}))\n",
        "print(\"Other candidates:\", candidate_count({\"source\":\"other\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYTzXpR4Dr_d",
        "outputId": "62465a7d-4459-4af6-a79d-56617e17f53a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All candidates: 6\n",
            "PG candidates: 1\n",
            "Other candidates: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this demonstrates (say this explicitly)\n",
        "\n",
        "Filtering happens before similarity search\n",
        "\n",
        "Smaller candidate pool → lower latency\n",
        "\n",
        "With tiny data, difference is small\n",
        "\n",
        "At scale, this is non-optional\n",
        "\n",
        "Use this sentence verbatim if you want:\n",
        "\n",
        "“Metadata filtering doesn’t change ranking logic; it shrinks the search space.\n",
        "The speedup compounds as the corpus grows.”"
      ],
      "metadata": {
        "id": "HmSlsBv8EcBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHUNKING STRATEGY TUNING + INCREMENTAL UPDATES (HF embeddings + BM25) ---\n",
        "# Assumes you already have:\n",
        "#   client (chromadb.PersistentClient)\n",
        "#   dense_ef (SentenceTransformerEmbeddingFunction)\n",
        "#   collection (current collection)\n",
        "#   tokenize(), rebuild_bm25() from earlier\n",
        "#   bm25, bm25_ids, bm25_docs already built at least once\n",
        "\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Chunking strategy tuning\n",
        "# ----------------------------\n",
        "\n",
        "def chunk_text(s, chunk_size=1200, overlap=200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(s)\n",
        "    while start < n:\n",
        "        end = min(n, start + chunk_size)\n",
        "        chunk = s[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "        if end == n:\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def create_collection_from_text(\n",
        "    client,\n",
        "    name,\n",
        "    text,\n",
        "    chunk_size,\n",
        "    overlap,\n",
        "    source=\"pg\",\n",
        "    embedding_function=None,\n",
        "    batch_size=64,\n",
        "    wipe_if_exists=True,\n",
        "):\n",
        "    # recreate the collection for a clean comparison\n",
        "    if wipe_if_exists:\n",
        "        try:\n",
        "            client.delete_collection(name)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    coll = client.get_or_create_collection(name=name, embedding_function=embedding_function)\n",
        "\n",
        "    chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
        "    ids = [f\"{source}_{i}\" for i in range(len(chunks))]\n",
        "    metas = [{\"source\": source, \"chunk\": i, \"chunk_size\": chunk_size, \"overlap\": overlap} for i in range(len(chunks))]\n",
        "\n",
        "    # batched add to avoid Colab spikes\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        coll.add(\n",
        "            ids=ids[i:i+batch_size],\n",
        "            documents=chunks[i:i+batch_size],\n",
        "            metadatas=metas[i:i+batch_size],\n",
        "        )\n",
        "    return coll, len(chunks)\n",
        "\n",
        "\n",
        "def timed_dense_query(coll, query, k=5, repeat=5):\n",
        "    # warmup\n",
        "    coll.query(query_texts=[query], n_results=1)\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(repeat):\n",
        "        r = coll.query(query_texts=[query], n_results=k)\n",
        "    t1 = time.perf_counter()\n",
        "    return (t1 - t0) * 1000 / repeat, r\n",
        "\n",
        "\n",
        "# Load the essay text once (streaming, capped)\n",
        "ds = load_dataset(\"chromadb/paul_graham_essay\", split=\"data\", streaming=True)\n",
        "MAX_CHARS = 250_000\n",
        "texts, total = [], 0\n",
        "for row in ds:\n",
        "    t = row.get(\"text\") or next(v for v in row.values() if isinstance(v, str))\n",
        "    if not t:\n",
        "        continue\n",
        "    if total + len(t) > MAX_CHARS:\n",
        "        t = t[: max(0, MAX_CHARS - total)]\n",
        "    texts.append(t)\n",
        "    total += len(t)\n",
        "    if total >= MAX_CHARS:\n",
        "        break\n",
        "essay_text = \"\\n\".join(texts)\n",
        "\n",
        "# Build 3 variants to compare\n",
        "variants = [\n",
        "    (\"pg_cs400_ov50\",  400,  50),\n",
        "    (\"pg_cs800_ov100\", 800,  100),\n",
        "    (\"pg_cs1200_ov200\",1200, 200),\n",
        "]\n",
        "\n",
        "query = \"startup advice about writing and learning\"\n",
        "results = []\n",
        "\n",
        "for name, cs, ov in variants:\n",
        "    coll, n_chunks = create_collection_from_text(\n",
        "        client=client,\n",
        "        name=name,\n",
        "        text=essay_text,\n",
        "        chunk_size=cs,\n",
        "        overlap=ov,\n",
        "        source=\"pg\",\n",
        "        embedding_function=dense_ef,\n",
        "        batch_size=64,\n",
        "        wipe_if_exists=True,\n",
        "    )\n",
        "    ms, r = timed_dense_query(coll, query, k=5, repeat=5)\n",
        "    results.append((name, cs, ov, n_chunks, ms, r[\"ids\"][0]))\n",
        "\n",
        "print(\"Chunking comparison (name, chunk_size, overlap, #chunks, avg_ms, top_ids):\")\n",
        "for row in results:\n",
        "    print(row[0], row[1], row[2], row[3], f\"{row[4]:.2f}ms\", row[5][:3])\n",
        "\n",
        "\n",
        "# -------------------------------------\n",
        "# 2) Incremental updates (no full reindex)\n",
        "# -------------------------------------\n",
        "# Pattern:\n",
        "#   - Add only new docs (new IDs)\n",
        "#   - Chroma embeds only the new docs (because embedding_function is attached)\n",
        "#   - Rebuild BM25 (or incrementally update; rebuild is simplest)\n",
        "\n",
        "def incremental_add_documents(coll, new_documents, source=\"daily\", start_id=0, batch_size=64):\n",
        "    ids = [f\"{source}_{start_id+i}\" for i in range(len(new_documents))]\n",
        "    metas = [{\"source\": source, \"chunk\": start_id+i} for i in range(len(new_documents))]\n",
        "\n",
        "    for i in range(0, len(new_documents), batch_size):\n",
        "        coll.add(\n",
        "            ids=ids[i:i+batch_size],\n",
        "            documents=new_documents[i:i+batch_size],\n",
        "            metadatas=metas[i:i+batch_size],\n",
        "        )\n",
        "    return ids\n",
        "\n",
        "\n",
        "# Example: simulate \"daily new docs\"\n",
        "new_docs_day1 = [\n",
        "    \"Daily update: We shipped a new feature that improves retrieval quality for exact names.\",\n",
        "    \"Daily update: Hybrid retrieval is now default for compliance-sensitive workflows.\",\n",
        "    \"Daily update: Metadata filters reduced latency by restricting the candidate pool.\",\n",
        "]\n",
        "\n",
        "before = collection.count()\n",
        "added_ids = incremental_add_documents(collection, new_docs_day1, source=\"daily\", start_id=0, batch_size=16)\n",
        "after = collection.count()\n",
        "print(\"Count before:\", before, \"after:\", after, \"added:\", len(added_ids))\n",
        "\n",
        "# Rebuild lexical index after updates (simple + standard)\n",
        "bm25, bm25_ids, bm25_docs = rebuild_bm25(collection)\n",
        "print(\"BM25 rebuilt on docs:\", len(bm25_docs))\n",
        "\n",
        "\n",
        "# Optional: quick verification queries\n",
        "def bm25_search_ids(query, k=5):\n",
        "    if bm25 is None:\n",
        "        return []\n",
        "    scores = bm25.get_scores(tokenize(query))\n",
        "    ranked = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [bm25_ids[i] for i in ranked]\n",
        "\n",
        "print(\"Dense top ids:\", collection.query(query_texts=[\"metadata filters latency\"], n_results=5)[\"ids\"][0])\n",
        "print(\"Lexical top ids:\", bm25_search_ids(\"metadata filters latency\", k=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiYwuGWOFNid",
        "outputId": "405695a1-db54-4936-b5ce-4080b131a884"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking comparison (name, chunk_size, overlap, #chunks, avg_ms, top_ids):\n",
            "pg_cs400_ov50 400 50 1 17.54ms ['pg_0']\n",
            "pg_cs800_ov100 800 100 1 23.76ms ['pg_0']\n",
            "pg_cs1200_ov200 1200 200 1 29.91ms ['pg_0']\n",
            "Count before: 6 after: 9 added: 3\n",
            "BM25 rebuilt on docs: 9\n",
            "Dense top ids: ['daily_2', 'daily_1', 'daily_0', 'other_4', 'other_2']\n",
            "Lexical top ids: ['daily_2', 'pg_0', 'other_0', 'other_1', 'other_2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDEi9CUuFOHr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}