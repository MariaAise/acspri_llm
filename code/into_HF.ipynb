{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwYYTaUS2ZFvatd/jHR0BZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a2aa94c8bcb4e3fa3aa8f805922bd38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6143033e18334a95b26d0aa8018fdcab",
              "IPY_MODEL_6193d95829ca400b8e3cb47e7504d80e",
              "IPY_MODEL_39a96d50d0ce4ba38fb45fa8539edb09"
            ],
            "layout": "IPY_MODEL_2d29d49aa4f440e3b02e950abece8ae3"
          }
        },
        "6143033e18334a95b26d0aa8018fdcab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cba9c40259fb4a1881e3894536c9b099",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b9860e9c278649678769a4bd36bc3133",
            "value": "README.md:‚Äá"
          }
        },
        "6193d95829ca400b8e3cb47e7504d80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34939b346b114f029e088c0aa02d57dd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e62f7c935eab4c64af3a6813726290ee",
            "value": 1
          }
        },
        "39a96d50d0ce4ba38fb45fa8539edb09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b41cb7473a724a5daf1a228860f32d0b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d49c127780d24be7b2b8ed814599d41d",
            "value": "‚Äá7.46k/?‚Äá[00:00&lt;00:00,‚Äá431kB/s]"
          }
        },
        "2d29d49aa4f440e3b02e950abece8ae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cba9c40259fb4a1881e3894536c9b099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9860e9c278649678769a4bd36bc3133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34939b346b114f029e088c0aa02d57dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e62f7c935eab4c64af3a6813726290ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b41cb7473a724a5daf1a228860f32d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d49c127780d24be7b2b8ed814599d41d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaAise/acspri_llm/blob/main/code/into_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face in Practice: Multi-task Tutorial (Datasets + Transformers + Evaluate + Trackio + Gradio)\n",
        "\n",
        "-----------\n",
        "> In this course, you primarily interact with **Transformers** and **Datasets**.\n",
        ">\n",
        "> Other Hugging Face libraries exist to support scaling, evaluation, and fine-tuning, but they are not required for basic LLM usage.\n",
        "\n",
        "\n",
        "## Hugging Face Libraries ‚Äî API References\n",
        "\n",
        "### ü§ó Transformers\n",
        "\n",
        "**Purpose:** Pretrained models, pipelines, inference, fine-tuning\n",
        "\n",
        "API documentation:\n",
        "[https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)\n",
        "\n",
        "Pipelines (recommended entry point):\n",
        "[https://huggingface.co/docs/transformers/main/en/pipeline_tutorial](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial)\n",
        "\n",
        "Python API reference:\n",
        "[https://huggingface.co/docs/transformers/main/en/api_reference](https://huggingface.co/docs/transformers/main/en/api_reference)\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ó Datasets\n",
        "\n",
        "**Purpose:** Loading, processing, and managing text (and multimodal) datasets\n",
        "\n",
        "API documentation:\n",
        "[https://huggingface.co/docs/datasets/index](https://huggingface.co/docs/datasets/index)\n",
        "\n",
        "Python API reference:\n",
        "[https://huggingface.co/docs/datasets/main/en/package_reference](https://huggingface.co/docs/datasets/main/en/package_reference)\n",
        "\n",
        "Loading datasets guide:\n",
        "[https://huggingface.co/docs/datasets/loading](https://huggingface.co/docs/datasets/loading)\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ó Tokenizers\n",
        "\n",
        "**Purpose:** Fast tokenization backend used by Transformers (mostly implicit use)\n",
        "\n",
        "API documentation:\n",
        "[https://huggingface.co/docs/tokenizers/index](https://huggingface.co/docs/tokenizers/index)\n",
        "\n",
        "Python API reference:\n",
        "[https://huggingface.co/docs/tokenizers/python/latest/](https://huggingface.co/docs/tokenizers/python/latest/)\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ó Evaluate\n",
        "\n",
        "**Purpose:** Standardised evaluation metrics for NLP and ML tasks\n",
        "\n",
        "API documentation:\n",
        "[https://huggingface.co/docs/evaluate/index](https://huggingface.co/docs/evaluate/index)\n",
        "\n",
        "Python API reference:\n",
        "[https://huggingface.co/docs/evaluate/package_reference](https://huggingface.co/docs/evaluate/package_reference)\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ó PEFT (Parameter-Efficient Fine-Tuning)\n",
        "\n",
        "**Purpose:** LoRA and other lightweight fine-tuning methods\n",
        "\n",
        "API documentation:\n",
        "[https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index)\n",
        "\n",
        "Python API reference:\n",
        "[https://huggingface.co/docs/peft/package_reference](https://huggingface.co/docs/peft/package_reference)\n",
        "\n",
        "(Used later when discussing fine-tuning conceptually.)\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ó Hub (Model & Dataset Hosting)\n",
        "\n",
        "**Purpose:** Model, dataset, and space hosting; authentication\n",
        "\n",
        "Hub documentation:\n",
        "[https://huggingface.co/docs/huggingface_hub/index](https://huggingface.co/docs/huggingface_hub/index)\n",
        "\n",
        "Python API reference:\n",
        "[https://huggingface.co/docs/huggingface_hub/package_reference](https://huggingface.co/docs/huggingface_hub/package_reference)\n",
        "\n",
        "---\n",
        "\n",
        "## Setup (Colab)\n",
        "\n",
        "### 1. Install"
      ],
      "metadata": {
        "id": "ioHuiSnB8F12"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkNA4SHMxXad",
        "outputId": "ac94444a-ee08-4800-c93e-6c21c57628fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U datasets transformers evaluate accelerate gradio trackio sentence-transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Imports + device"
      ],
      "metadata": {
        "id": "bipnfPoT8Qrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K9XPdUGv8TAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, numpy as np\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "\n",
        "import evaluate\n",
        "import trackio as wandb  # Trackio is wandb-compatible. :contentReference[oaicite:0]{index=0}\n",
        "\n",
        "DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"GPU available:\", torch.cuda.is_available(), \"DEVICE:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3eDqWza8cWq",
        "outputId": "dfdceeba-a495-44b7-b637-9def775ec95f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: False DEVICE: -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3 Trackio: initialize a project\n",
        "\n",
        "Trackio logs runs locally by default and you can launch a local dashboard via `trackio show`."
      ],
      "metadata": {
        "id": "ctBW04r89fgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT = \"hf-multitask-workshop\"\n",
        "\n",
        "def start_run(run_name: str, config: dict):\n",
        "    wandb.init(project=PROJECT, name=run_name, config=config)\n",
        "\n",
        "def log_metrics(metrics: dict, step: int | None = None):\n",
        "    if step is None:\n",
        "        wandb.log(metrics)\n",
        "    else:\n",
        "        wandb.log(metrics | {\"step\": step})\n",
        "\n",
        "def end_run():\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "jamE0GSE9xM_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with datasets\n",
        "\n",
        "Working with good data is the key, especially at early stage as you learn to work with Python.\n",
        "\n",
        "Hugging Face provides access to some best quality classic datasets that allow you to learn in the smoothest way.\n",
        "\n",
        "Data is available across all the modalities, such as text, image, audio, video as well as Hugging Face provides convenient tools to work with the data.\n",
        "\n",
        "\n",
        "[DatasetInfo](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.DatasetInfo): provides general info about datasets and is a good place to start\n",
        "\n",
        "Datasets can be heavy and committing to downloading them can be challenging: Use the `load_dataset_builder()` function to load a dataset builder and inspect a dataset‚Äôs attributes without committing to downloading it:"
      ],
      "metadata": {
        "id": "J4hgI8be-wh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset_builder\n",
        "ds_builder = load_dataset_builder(\"cornell-movie-review-data/rotten_tomatoes\")\n",
        "\n",
        "ds_builder.info.description\n",
        "\n",
        "ds_builder.info.features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "9a2aa94c8bcb4e3fa3aa8f805922bd38",
            "6143033e18334a95b26d0aa8018fdcab",
            "6193d95829ca400b8e3cb47e7504d80e",
            "39a96d50d0ce4ba38fb45fa8539edb09",
            "2d29d49aa4f440e3b02e950abece8ae3",
            "cba9c40259fb4a1881e3894536c9b099",
            "b9860e9c278649678769a4bd36bc3133",
            "34939b346b114f029e088c0aa02d57dd",
            "e62f7c935eab4c64af3a6813726290ee",
            "b41cb7473a724a5daf1a228860f32d0b",
            "d49c127780d24be7b2b8ed814599d41d"
          ]
        },
        "id": "g8YJPvb3-zqY",
        "outputId": "dcc30a0b-0243-4820-9f6a-cea1e4aa212f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a2aa94c8bcb4e3fa3aa8f805922bd38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': Value('string'), 'label': ClassLabel(names=['neg', 'pos'])}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splits**\n",
        "A split is a specific subset of a dataset like train and test. List a dataset‚Äôs split names with the get_dataset_split_names() function:"
      ],
      "metadata": {
        "id": "8q8FNims-3sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_split_names\n",
        "\n",
        "get_dataset_split_names(\"cornell-movie-review-data/rotten_tomatoes\")"
      ],
      "metadata": {
        "id": "XA_xEZN--6np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can load a specific split with the split parameter. Loading a dataset split returns a Dataset object:"
      ],
      "metadata": {
        "id": "d6q9IBXa-80R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "0AakmvBc--zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don‚Äôt specify a split, ü§ó Datasets returns a DatasetDict object instead:\n"
      ],
      "metadata": {
        "id": "ffD-cWCz_BpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")"
      ],
      "metadata": {
        "id": "JV4O3_1N_D_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some datasets contain several sub-datasets. For example, the `MInDS-14` dataset has several sub-datasets, each one containing audio data in a different language. These sub-datasets are known as configurations or subsets, and you must explicitly select one when loading the dataset. If you don‚Äôt provide a configuration name, ü§ó Datasets will raise a ValueError and remind you to choose a configuration.\n",
        "\n",
        "Use the `get_dataset_config_names()` function to retrieve a list of all the possible configurations available to your dataset:"
      ],
      "metadata": {
        "id": "i2iPzsBD_GYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "\n",
        "configs = get_dataset_config_names(\"PolyAI/minds14\")\n",
        "print(configs)"
      ],
      "metadata": {
        "id": "b8JkBvaF_K1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "mindsFR = load_dataset(\"PolyAI/minds14\", \"fr-FR\", split=\"train\")"
      ],
      "metadata": {
        "id": "KwZmslUn_ORG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterable vs regular datasets\n",
        "\n",
        "There are two types of dataset objects, a **regular Dataset** and then an ‚ú® **IterableDataset** ‚ú®.\n",
        "\n",
        "A Dataset provides fast **random** access to the rows, and memory-mapping so that loading even large datasets only uses a relatively small amount of device memory. But for really, really big datasets that won‚Äôt even fit on disk or in memory, an IterableDataset allows you to access and use the dataset without waiting for it to download completely."
      ],
      "metadata": {
        "id": "H3m34hTP_KZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "When you load a dataset split, you‚Äôll get a `Dataset` object."
      ],
      "metadata": {
        "id": "2PVJHLrn_Tee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")"
      ],
      "metadata": {
        "id": "Y54qapoD_WOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Indexing**\n",
        "\n",
        "A Dataset contains columns of data, and each column can be a different type of data. The **index**, or **axis label**, is used to access examples from the dataset."
      ],
      "metadata": {
        "id": "7VbBwN69_ZnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "G2_RO13N_cJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to access the last one: -\n"
      ],
      "metadata": {
        "id": "a4EMkuWM_fVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[-1]"
      ],
      "metadata": {
        "id": "fuEO_8s__iC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ],
      "metadata": {
        "id": "V8M-7Yyh_koV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing by the column name returns a list of all the values in the column:"
      ],
      "metadata": {
        "id": "uZMlcCa-_pBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"text\"]"
      ],
      "metadata": {
        "id": "vIYlmfzf_r3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can combine row and column name indexing to return a specific value at a position:"
      ],
      "metadata": {
        "id": "3I_hXrbV_v-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"text\"]"
      ],
      "metadata": {
        "id": "qZ0NXfJC_yMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Slicing**\n",
        "\n",
        "Slicing returns a slice - or subset - of the dataset, which is useful for viewing several rows at once. To slice a dataset, use the : operator to specify a range of positions."
      ],
      "metadata": {
        "id": "IkgsxdfM_1bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:3]\n",
        "\n",
        "dataset[3:6]"
      ],
      "metadata": {
        "id": "KmkRkvWJ_4JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IterableDataset\n",
        "\n",
        "An IterableDataset is loaded when you set the streaming parameter to True in load_dataset():"
      ],
      "metadata": {
        "id": "NxnQHf40_7Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "iterable_dataset = load_dataset(\"ethz/food101\", split=\"train\", streaming=True)\n",
        "for example in iterable_dataset:\n",
        "    print(example)\n",
        "    break"
      ],
      "metadata": {
        "id": "ok507tY1_-gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "An IterableDataset‚Äôs behavior is different from a regular Dataset. You don‚Äôt get random access to examples in an IterableDataset. Instead, you should iterate over its elements, for example, by calling next(iter()) or with a for loop to return the next item from the IterableDataset:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M6ep4jryAChD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(iterable_dataset))\n",
        "\n",
        "for example in iterable_dataset:\n",
        "    print(example)\n",
        "    break"
      ],
      "metadata": {
        "id": "zBRqDaMoAFxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## HF Tasks\n",
        "---\n",
        "\n",
        "## 1) Text classification (AG News) + Evaluate + Trackio\n",
        "\n",
        "**Dataset:** `ag_news`\n",
        "**Task:** supervised text classification (4 classes)\n",
        "\n",
        "### 1.1 Load a small slice"
      ],
      "metadata": {
        "id": "DzyvDVlpAVB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"ag_news\")\n",
        "print(ds)\n",
        "\n",
        "# Small slices for speed\n",
        "train_small = ds[\"train\"].shuffle(seed=42).select(range(600))\n",
        "test_small  = ds[\"test\"].shuffle(seed=42).select(range(300))\n",
        "\n",
        "label_names = ds[\"train\"].features[\"label\"].names\n",
        "label_names"
      ],
      "metadata": {
        "id": "Q8IgxjFVAX18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Baseline model (no training): `pipeline(\"text-classification\")`\n",
        "\n",
        "Pick a fast checkpoint that supports 4-way news classification. This one is commonly used for AG News:"
      ],
      "metadata": {
        "id": "zulgkCUBAbk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"textattack/bert-base-uncased-ag-news\"\n",
        "\n",
        "clf = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=MODEL,\n",
        "    tokenizer=MODEL,\n",
        "    device=DEVICE,\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        ")"
      ],
      "metadata": {
        "id": "i0yFV2eGAdpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Predict + evaluate"
      ],
      "metadata": {
        "id": "ixJ3nSkEAggj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = evaluate.load(\"accuracy\")\n",
        "f1  = evaluate.load(\"f1\")\n",
        "\n",
        "def predict_batch(texts):\n",
        "    out = clf(texts, batch_size=16)\n",
        "    # pipeline returns labels like \"LABEL_0\"... map to id\n",
        "    pred_ids = [int(o[\"label\"].split(\"_\")[-1]) for o in out]\n",
        "    scores   = [float(o[\"score\"]) for o in out]\n",
        "    return pred_ids, scores\n",
        "\n",
        "texts = test_small[\"text\"]\n",
        "y_true = test_small[\"label\"]\n",
        "\n",
        "y_pred, y_score = predict_batch(texts)\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\": acc.compute(predictions=y_pred, references=y_true)[\"accuracy\"],\n",
        "    \"f1_macro\": f1.compute(predictions=y_pred, references=y_true, average=\"macro\")[\"f1\"],\n",
        "    \"mean_confidence\": float(np.mean(y_score)),\n",
        "}\n",
        "metrics"
      ],
      "metadata": {
        "id": "tatMRWfoAjNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Log to Trackio"
      ],
      "metadata": {
        "id": "0zIYpeO5AmJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_run(\n",
        "    run_name=\"ag_news_baseline_inference\",\n",
        "    config={\"dataset\":\"ag_news\", \"model\":MODEL, \"max_length\":256, \"batch_size\":16}\n",
        ")\n",
        "\n",
        "log_metrics(metrics)\n",
        "\n",
        "# Log a few examples as a lightweight artifact pattern\n",
        "examples = []\n",
        "for i in range(5):\n",
        "    examples.append({\n",
        "        \"text\": texts[i][:280],\n",
        "        \"true\": label_names[y_true[i]],\n",
        "        \"pred\": label_names[y_pred[i]],\n",
        "        \"score\": y_score[i],\n",
        "    })\n",
        "wandb.log({\"examples\": examples})\n",
        "\n",
        "end_run()\n",
        "print(\"Run logged. To view dashboard in a terminal: trackio show\")  # :contentReference[oaicite:2]{index=2}"
      ],
      "metadata": {
        "id": "0iYMAdJTApM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2) Zero-shot text classification (AG News ‚Üí your own labels)\n",
        "\n",
        "**Task:** zero-shot classification (no labels in training; you provide candidate labels)\n",
        "\n",
        "### 2.1 Build a zero-shot pipeline"
      ],
      "metadata": {
        "id": "O1AS76vpAtRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ZSHOT_MODEL = \"facebook/bart-large-mnli\"\n",
        "\n",
        "zshot = pipeline(\n",
        "    task=\"zero-shot-classification\",\n",
        "    model=ZSHOT_MODEL,\n",
        "    device=DEVICE,\n",
        ")"
      ],
      "metadata": {
        "id": "Jnw5uJY0AwZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Define labels and run"
      ],
      "metadata": {
        "id": "1F2nSGZnAypS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_labels = [\"politics\", \"business\", \"sports\", \"technology\"]\n",
        "\n",
        "sample_texts = test_small[\"text\"][:20]\n",
        "out = zshot(sample_texts, candidate_labels=candidate_labels, batch_size=8)\n",
        "\n",
        "pred_labels = [o[\"labels\"][0] for o in out]\n",
        "pred_scores = [float(o[\"scores\"][0]) for o in out]\n",
        "\n",
        "list(zip(pred_labels[:5], pred_scores[:5]))"
      ],
      "metadata": {
        "id": "8PFaWur_A1lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Evaluate by mapping your labels ‚Üí dataset labels\n",
        "\n",
        "If you align labels to AG News classes, you can evaluate. Here is one simple mapping:"
      ],
      "metadata": {
        "id": "q5EiGjkbA4PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AG News labels: [\"World\",\"Sports\",\"Business\",\"Sci/Tech\"]\n",
        "ag_to_custom = {\n",
        "    \"World\": \"politics\",\n",
        "    \"Sports\": \"sports\",\n",
        "    \"Business\": \"business\",\n",
        "    \"Sci/Tech\": \"technology\",\n",
        "}\n",
        "custom_to_ag = {v:k for k,v in ag_to_custom.items()}\n",
        "\n",
        "y_true_names = [label_names[i] for i in test_small[\"label\"][:20]]\n",
        "y_true_custom = [ag_to_custom[n] for n in y_true_names]\n",
        "acc_custom = sum(p==t for p,t in zip(pred_labels, y_true_custom)) / len(y_true_custom)\n",
        "acc_custom"
      ],
      "metadata": {
        "id": "3FHja2cfA8GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Log the run"
      ],
      "metadata": {
        "id": "FAmBZPtpA92-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_run(\n",
        "    run_name=\"ag_news_zero_shot\",\n",
        "    config={\"dataset\":\"ag_news\", \"model\":ZSHOT_MODEL, \"candidate_labels\":candidate_labels}\n",
        ")\n",
        "log_metrics({\"zero_shot_acc_on_20\": acc_custom, \"mean_top_score\": float(np.mean(pred_scores))})\n",
        "end_run()"
      ],
      "metadata": {
        "id": "TpIW0vpdBAH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3) Multi-label emotion (GoEmotions) with a zero-shot baseline\n",
        "\n",
        "**Dataset:** `go_emotions`\n",
        "**Task:** multi-label emotion (harder; good for ‚Äúwhy evaluation matters‚Äù)\n",
        "\n",
        "### 3.1 Load a small slice"
      ],
      "metadata": {
        "id": "UXjhYxjXBEj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "go = load_dataset(\"go_emotions\", \"raw\")\n",
        "go_small = go[\"test\"].shuffle(seed=42).select(range(120))\n",
        "\n",
        "# Dataset has many labels; for a clean demo pick a manageable subset\n",
        "emotion_labels = [\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"disgust\"]"
      ],
      "metadata": {
        "id": "_Ty0IIEcBGHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Zero-shot multi-label trick\n",
        "\n",
        "Zero-shot pipeline supports `multi_label=True`."
      ],
      "metadata": {
        "id": "uhYxN4QjBJ0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = go_small[\"text\"]\n",
        "out = zshot(texts, candidate_labels=emotion_labels, multi_label=True, batch_size=8)\n",
        "\n",
        "# Keep top-2 predicted emotions per text (demo choice)\n",
        "pred_top2 = [o[\"labels\"][:2] for o in out]\n",
        "pred_top2[:3]"
      ],
      "metadata": {
        "id": "ozxTaALbBJMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Log qualitative outputs (this is the point)"
      ],
      "metadata": {
        "id": "5sNrOZpGBM-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_run(\n",
        "    run_name=\"go_emotions_zero_shot_top2\",\n",
        "    config={\"dataset\":\"go_emotions\", \"model\":ZSHOT_MODEL, \"labels\":emotion_labels, \"topk\":2}\n",
        ")\n",
        "wandb.log({\"examples\": [{\"text\": texts[i][:240], \"pred_top2\": pred_top2[i]} for i in range(10)]})\n",
        "end_run()"
      ],
      "metadata": {
        "id": "iSL_b3k_BPU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4) Image classification (CIFAR-10) + Gradio demo\n",
        "\n",
        "**Dataset:** `cifar10`\n",
        "**Task:** image classification (instant visual engagement)\n",
        "\n",
        "### 4.1 Load a small slice"
      ],
      "metadata": {
        "id": "lCaBkdZaBShw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar = load_dataset(\"cifar10\")\n",
        "cifar_small = cifar[\"test\"].shuffle(seed=42).select(range(60))\n",
        "cifar_labels = cifar[\"train\"].features[\"label\"].names\n",
        "cifar_labels"
      ],
      "metadata": {
        "id": "T90jKRfsBUuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Pipeline"
      ],
      "metadata": {
        "id": "PMnNG2lPBXWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_MODEL = \"google/vit-base-patch16-224\"\n",
        "\n",
        "img_clf = pipeline(\n",
        "    task=\"image-classification\",\n",
        "    model=IMG_MODEL,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "# quick sanity\n",
        "img = cifar_small[0][\"img\"]\n",
        "img_clf(img)[:3]"
      ],
      "metadata": {
        "id": "yY4Ys6KRBZTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Gradio app (image ‚Üí top-5 labels)"
      ],
      "metadata": {
        "id": "mpTjvlY0BcKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def classify_image(image):\n",
        "    preds = img_clf(image)\n",
        "    # return as nice table\n",
        "    return {p[\"label\"]: float(p[\"score\"]) for p in preds[:5]}\n",
        "\n",
        "demo1 = gr.Interface(\n",
        "    fn=classify_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=gr.Label(num_top_classes=5),\n",
        "    title=\"Image Classification (ViT)\",\n",
        "    description=\"Upload an image (CIFAR-10 works best for this demo).\"\n",
        ")\n",
        "\n",
        "demo1.launch(share=True)"
      ],
      "metadata": {
        "id": "QvFamXnjBeMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 5) Image captioning (Pok√©mon BLIP captions dataset) + ‚ÄúWOW‚Äù demo\n",
        "\n",
        "**Dataset:** `lambdalabs/pokemon-blip-captions`\n",
        "**Task:** image ‚Üí text captioning (high WOW, low effort)\n",
        "\n",
        "### 5.1 Load a small slice"
      ],
      "metadata": {
        "id": "XLASscY6Bj2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poke = load_dataset(\"lambdalabs/pokemon-blip-captions\", split=\"train\")\n",
        "poke_small = poke.shuffle(seed=42).select(range(80))\n",
        "\n",
        "poke_small[0].keys()"
      ],
      "metadata": {
        "id": "YHXDKDiMBloE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Caption pipeline"
      ],
      "metadata": {
        "id": "WDu3xVN1BoLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CAPTION_MODEL = \"Salesforce/blip-image-captioning-base\"\n",
        "\n",
        "captioner = pipeline(\n",
        "    task=\"image-to-text\",\n",
        "    model=CAPTION_MODEL,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "# Try one image\n",
        "captioner(poke_small[0][\"image\"])[0]"
      ],
      "metadata": {
        "id": "CadIYGlUBqIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Gradio app (image ‚Üí caption)"
      ],
      "metadata": {
        "id": "1geqsHjwBsG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def caption_image(image):\n",
        "    out = captioner(image, max_new_tokens=30)\n",
        "    return out[0][\"generated_text\"]\n",
        "\n",
        "examples = [[poke_small[i][\"image\"]] for i in range(6)]\n",
        "\n",
        "demo2 = gr.Interface(\n",
        "    fn=caption_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=gr.Textbox(label=\"Caption\"),\n",
        "    examples=examples,\n",
        "    title=\"Image Captioning (BLIP)\",\n",
        "    description=\"Upload an image or click an example (Pok√©mon dataset).\"\n",
        ")\n",
        "demo2.launch(share=True)"
      ],
      "metadata": {
        "id": "P6AHd_0NBuFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Log a run (caption quality is qualitative)"
      ],
      "metadata": {
        "id": "DX4Hc2JfBx9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_run(\n",
        "    run_name=\"pokemon_captioning_blip\",\n",
        "    config={\"dataset\":\"lambdalabs/pokemon-blip-captions\", \"model\":CAPTION_MODEL, \"n_examples\":10}\n",
        ")\n",
        "samples = []\n",
        "for i in range(10):\n",
        "    cap = captioner(poke_small[i][\"image\"], max_new_tokens=30)[0][\"generated_text\"]\n",
        "    samples.append({\"dataset_caption\": poke_small[i][\"text\"], \"model_caption\": cap})\n",
        "wandb.log({\"caption_samples\": samples})\n",
        "end_run()"
      ],
      "metadata": {
        "id": "-fcAIxqhBz3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 6) Multimodal retrieval (text ‚Üî image) using CLIP embeddings\n",
        "\n",
        "**Goal:** Type a text query ‚Üí retrieve nearest images (very strong ‚Äúmagic‚Äù moment).\n",
        "\n",
        "### 6.1 Embed images + captions (small index)"
      ],
      "metadata": {
        "id": "xAoB0iIzCHBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from PIL import Image\n",
        "\n",
        "CLIP_MODEL = \"clip-ViT-B-32\"\n",
        "clip = SentenceTransformer(CLIP_MODEL)\n",
        "\n",
        "# Build a tiny retrieval index\n",
        "N = 120\n",
        "idx_ds = poke.shuffle(seed=0).select(range(N))\n",
        "\n",
        "images = [ex[\"image\"] for ex in idx_ds]\n",
        "captions = [ex[\"text\"] for ex in idx_ds]\n",
        "\n",
        "# Encode images and text into same embedding space\n",
        "img_emb = clip.encode(images, batch_size=16, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "txt_emb = clip.encode(captions, batch_size=32, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "img_emb.shape, txt_emb.shape"
      ],
      "metadata": {
        "id": "BoS_STEKCJHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Retrieval function"
      ],
      "metadata": {
        "id": "Paoyx3l8CL3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_images_by_text(query: str, k: int = 6):\n",
        "    q = clip.encode([query], normalize_embeddings=True, convert_to_numpy=True)[0]\n",
        "    scores = img_emb @ q  # cosine similarity (normalized)\n",
        "    topk = np.argsort(-scores)[:k]\n",
        "    return [images[i] for i in topk], [float(scores[i]) for i in topk], [captions[i] for i in topk]"
      ],
      "metadata": {
        "id": "SAD668zwCNai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Gradio app (text ‚Üí gallery)"
      ],
      "metadata": {
        "id": "4uGdL3t5CO65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gr_retrieve(query):\n",
        "    imgs, scores, caps = retrieve_images_by_text(query, k=8)\n",
        "    gallery = []\n",
        "    for im, s, c in zip(imgs, scores, caps):\n",
        "        gallery.append((im, f\"{s:.3f} | {c[:60]}\"))\n",
        "    return gallery\n",
        "\n",
        "demo3 = gr.Interface(\n",
        "    fn=gr_retrieve,\n",
        "    inputs=gr.Textbox(label=\"Text query\", placeholder=\"e.g., 'yellow mouse with lightning'\"),\n",
        "    outputs=gr.Gallery(label=\"Top matches\", columns=4, height=\"auto\"),\n",
        "    title=\"Text ‚Üí Image Retrieval (CLIP)\",\n",
        "    description=\"CLIP embeds text and images into the same space; nearest neighbors are semantically related.\"\n",
        ")\n",
        "demo3.launch(share=True)"
      ],
      "metadata": {
        "id": "vNf2jguXCQlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Trackio logging for retrieval (simple Recall@K demo)\n",
        "\n",
        "A quick, lightweight metric: for each dataset caption, does its own image appear in top-K when you query the caption?\n"
      ],
      "metadata": {
        "id": "GVKUYgYrCS7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(k=5, n=60):\n",
        "    hits = 0\n",
        "    for i in range(n):\n",
        "        query = captions[i]\n",
        "        q = clip.encode([query], normalize_embeddings=True, convert_to_numpy=True)[0]\n",
        "        scores = img_emb @ q\n",
        "        topk = set(np.argsort(-scores)[:k])\n",
        "        if i in topk:\n",
        "            hits += 1\n",
        "    return hits / n\n",
        "\n",
        "r5 = recall_at_k(k=5, n=60)\n",
        "r10 = recall_at_k(k=10, n=60)\n",
        "\n",
        "start_run(\n",
        "    run_name=\"pokemon_clip_retrieval\",\n",
        "    config={\"dataset\":\"lambdalabs/pokemon-blip-captions\", \"clip_model\":CLIP_MODEL, \"index_size\":N}\n",
        ")\n",
        "log_metrics({\"recall@5\": r5, \"recall@10\": r10})\n",
        "end_run()"
      ],
      "metadata": {
        "id": "Aj5H73q5CVKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Trackio dashboard\n",
        "\n",
        "From a terminal (or Colab cell with `!`):"
      ],
      "metadata": {
        "id": "EW6vVNUECYfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!trackio show"
      ],
      "metadata": {
        "id": "tRBBLujaCcO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally open a specific project:\n",
        "\n",
        "!trackio show --project \"hf-multitask-workshop\""
      ],
      "metadata": {
        "id": "i3nVb7TPChxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Note\n",
        "\n",
        "#### Weights & Biases (wandb): experiment tracking\n",
        "\n",
        "**What it is**\n",
        "A service + Python library for logging and comparing ML/LLM experiments (‚Äúruns‚Äù).\n",
        "\n",
        "**What you use it for in this course context**\n",
        "\n",
        "* Record parameters (model name, prompt version, temperature, dataset slice)\n",
        "* Log metrics (accuracy/F1, cost per 1k tokens, latency, hallucination rate)\n",
        "* Save artifacts (datasets, prompt templates, evaluation outputs)\n",
        "* Compare runs across prompt variants or model choices\n",
        "\n",
        "**Typical workflow**\n",
        "\n",
        "1. Start a run\n",
        "2. Log config + metrics during processing\n",
        "3. Review runs in the web UI to compare results and reproduce\n",
        "\n",
        "**Minimal example**\n",
        "\n",
        "```python\n",
        "!pip -q install wandb\n",
        "import wandb\n",
        "\n",
        "wandb.login()  # opens auth flow\n",
        "\n",
        "wandb.init(project=\"acspri-llm-tutorial\", config={\n",
        "    \"model\": \"gemini-1.5-flash\",\n",
        "    \"task\": \"sentiment\",\n",
        "    \"temperature\": 0.2\n",
        "})\n",
        "\n",
        "wandb.log({\"accuracy\": 0.81, \"avg_latency_s\": 0.42, \"usd_cost_est\": 0.18})\n",
        "wandb.finish()\n",
        "```\n",
        "\n",
        "**When to use**\n",
        "\n",
        "* Any time you run multiple prompt/model variants and need a record of what changed.\n",
        "* Any time you need reproducibility for a report/paper.\n",
        "\n",
        "---\n",
        "\n",
        "## Trackio: lightweight local tracking (no hosted dashboard required)\n",
        "\n",
        "**What it is**\n",
        "A small local-first tracking library for logging runs and metrics without relying on an external hosted platform.\n",
        "\n",
        "**What you use it for in this course context**\n",
        "\n",
        "* Track prompt/model variants locally inside Colab\n",
        "* Log metrics and outputs to files you can save to Drive/GitHub\n",
        "* Keep an audit trail for research workflows when external services are not allowed\n",
        "\n",
        "**Typical workflow**\n",
        "\n",
        "1. Create a local run directory\n",
        "2. Log config + metrics + outputs to JSON/CSV\n",
        "3. Save the folder to Drive or commit to GitHub\n",
        "\n",
        "**Minimal ‚Äútrackio-style‚Äù pattern (local run folder)**\n",
        "If you do not want to depend on any particular tracking vendor, teach this neutral pattern:\n",
        "\n",
        "```python\n",
        "import json, time\n",
        "from pathlib import Path\n",
        "\n",
        "RUNS = Path(\"/content/outputs/runs\")\n",
        "RUNS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "run_dir = RUNS / run_id\n",
        "run_dir.mkdir()\n",
        "\n",
        "config = {\"model\": \"gemini-1.5-flash\", \"temperature\": 0.2, \"task\": \"sentiment\"}\n",
        "(run_dir / \"config.json\").write_text(json.dumps(config, indent=2))\n",
        "\n",
        "metrics = {\"accuracy\": 0.81, \"avg_latency_s\": 0.42, \"usd_cost_est\": 0.18}\n",
        "(run_dir / \"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
        "\n",
        "print(\"Saved run to:\", run_dir)\n",
        "```\n",
        "\n",
        "**When to use**\n",
        "\n",
        "* When you need tracking but cannot use hosted tools.\n",
        "* When you want outputs you can bundle as research artifacts (Drive/GitHub).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GV48oFWc9SXo"
      }
    }
  ]
}